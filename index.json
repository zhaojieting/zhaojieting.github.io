[{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6af6f8ffa007202518b782306cb9478a","permalink":"https://robot-vision-and-navigation-lab.github.io/author/changfei-fu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/changfei-fu/","section":"authors","summary":"","tags":null,"title":"Changfei Fu","type":"authors"},{"authors":null,"categories":null,"content":"I was a Professor in the Department of Computing Science at the University of Alberta. I received my BSc degree from Northeastern University (USA) and my PhD from Purdue University. After a year of post-doctoral training at the University of Pennsylvania, I joined the U of A in 1988 where I currently hold an adjunct appointment.\nMy research interests include robotics, computer vision, and image processing. I have worked in a number of areas in robotics and, for the past 10+ years, my focus has been on visual robot navigation. In 2003-17, with support from the federal and provinncial goverments, I held an NSERC Industrial Research Chair to conduct research in computer vision and image processing that addresses the practical challenges facing Alberta’s mining industry. As a member of the NSERC Strategic Network on Robotics (NCRN) I work closely with Canadian academic colleagues and industrial partners on mobile robotics research. Within the international robotics community, my activities include a variety of roles in ICRA and IROS communities, as well as other IEEE RAS sponsored conferences.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9ba420b8ac562a35c57569aeb0c48f01","permalink":"https://robot-vision-and-navigation-lab.github.io/author/hong-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hong-zhang/","section":"authors","summary":"I was a Professor in the Department of Computing Science at the University of Alberta. I received my BSc degree from Northeastern University (USA) and my PhD from Purdue University. After a year of post-doctoral training at the University of Pennsylvania, I joined the U of A in 1988 where I currently hold an adjunct appointment.","tags":null,"title":"Hong Zhang","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3dd3edd38efbd636db31f9e083b88ff6","permalink":"https://robot-vision-and-navigation-lab.github.io/author/li-he/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/li-he/","section":"authors","summary":"","tags":null,"title":"Li He","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"94c29cab9a33bd653548dfb123e435fd","permalink":"https://robot-vision-and-navigation-lab.github.io/author/weinan-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/weinan-chen/","section":"authors","summary":"","tags":null,"title":"Weinan Chen","type":"authors"},{"authors":null,"categories":null,"content":"Table of Contents  About Videos Releated Publications    About Assistive robot systems have been developed to help people accomplish daily manipulation tasks especially for those with disabilities, where scene understanding plays a crucial role in enabling robots to interpret the surroundings and behave accordingly. However, most of the current systems approach scene understanding without considering the functional dependencies between objects. In this research, we augment an assistive robotic arm system with an end-to-end semantic relationship reasoning model. It incorporates functional relationships between pairs of objects for semantic scene understanding. To ensure good generalization to unseen objects and relationships, the model works in a category-agnostic manner. We further demonstrate the effectiveness of our pipeline by integrating it with a symbolic planner for goal-oriented, multi-step manipulation task.\nVideos Releated Publications Relationship Oriented Semantic Scene Understanding for Daily Manipulation Tasks (submitted to IROS 2022, under review)\n","date":1655536214,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655536214,"objectID":"b05779546f5a3b044da8cfca91e969ad","permalink":"https://robot-vision-and-navigation-lab.github.io/project/relationship-oriented-semantic-scene-understanding-for-daily-manpulation-tasks/","publishdate":"2022-06-18T07:10:14.697Z","relpermalink":"/project/relationship-oriented-semantic-scene-understanding-for-daily-manpulation-tasks/","section":"project","summary":"Table of Contents  About Videos Releated Publications    About Assistive robot systems have been developed to help people accomplish daily manipulation tasks especially for those with disabilities, where scene understanding plays a crucial role in enabling robots to interpret the surroundings and behave accordingly.","tags":null,"title":"Relationship Oriented Semantic Scene Understanding for Daily Manpulation Tasks","type":"project"},{"authors":[],"categories":[],"content":"  Monocular person following (MPF) is a capability that supports many useful applications of a mobile robot. However, existing MPF solutions are not completely satisfactory. Firstly, they often fail to track the target at a close distance either because they are based on visual servo or they need the observation of the full body by the robot. Secondly, their target Re-IDentiﬁcation (Re-ID) abilities are weak in cases of target appearance change and highly similar appearance of distracting people. To remove the assumption of full-body observation, we propose a width-based tracking module, which relies on the target width, which can be observed even at a close distance. For handling issues related to appearance variation, we use a global CNN (convolutional neural network) descriptor to represent the target and a ridge regression model to learn a target appearance model online. We adopt a sampling strategy for online classiﬁer learning, in which both long-term and short-term samples are involved. We evaluate our method in two datasets including a public person following dataset and a custom-built with challenging target appearance and target distance. Our method achieves state-of-the-art (SOTA) results on both datasets. The code and dataset of our work in this research are publicly available in https://github.com/MedlarTea/MPF GRR SLT.\n","date":1651764665,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651764665,"objectID":"a40abd9d65ab68fc3390386b4df3a664","permalink":"https://robot-vision-and-navigation-lab.github.io/post/test_news_3/","publishdate":"2022-05-05T23:31:05+08:00","relpermalink":"/post/test_news_3/","section":"post","summary":"Following Closely: A Robust Monocular Person Following System for Mobile Robot","tags":[],"title":"A New CVPR Article","type":"post"},{"authors":[],"categories":[],"content":"  Monocular person following (MPF) is a capability that supports many useful applications of a mobile robot. However, existing MPF solutions are not completely satisfactory. Firstly, they often fail to track the target at a close distance either because they are based on visual servo or they need the observation of the full body by the robot. Secondly, their target Re-IDentiﬁcation (Re-ID) abilities are weak in cases of target appearance change and highly similar appearance of distracting people. To remove the assumption of full-body observation, we propose a width-based tracking module, which relies on the target width, which can be observed even at a close distance. For handling issues related to appearance variation, we use a global CNN (convolutional neural network) descriptor to represent the target and a ridge regression model to learn a target appearance model online. We adopt a sampling strategy for online classiﬁer learning, in which both long-term and short-term samples are involved. We evaluate our method in two datasets including a public person following dataset and a custom-built with challenging target appearance and target distance. Our method achieves state-of-the-art (SOTA) results on both datasets. The code and dataset of our work in this research are publicly available in https://github.com/MedlarTea/MPF GRR SLT.\n","date":1651764665,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651764665,"objectID":"7dd9328d5578c48f7884daf419eae54f","permalink":"https://robot-vision-and-navigation-lab.github.io/post/test_news_2/","publishdate":"2022-05-05T23:31:05+08:00","relpermalink":"/post/test_news_2/","section":"post","summary":"Following Closely: A Robust Monocular Person Following System for Mobile Robot","tags":[],"title":"A New ICRA Article","type":"post"},{"authors":[],"categories":[],"content":"  Monocular person following (MPF) is a capability that supports many useful applications of a mobile robot. However, existing MPF solutions are not completely satisfactory. Firstly, they often fail to track the target at a close distance either because they are based on visual servo or they need the observation of the full body by the robot. Secondly, their target Re-IDentiﬁcation (Re-ID) abilities are weak in cases of target appearance change and highly similar appearance of distracting people. To remove the assumption of full-body observation, we propose a width-based tracking module, which relies on the target width, which can be observed even at a close distance. For handling issues related to appearance variation, we use a global CNN (convolutional neural network) descriptor to represent the target and a ridge regression model to learn a target appearance model online. We adopt a sampling strategy for online classiﬁer learning, in which both long-term and short-term samples are involved. We evaluate our method in two datasets including a public person following dataset and a custom-built with challenging target appearance and target distance. Our method achieves state-of-the-art (SOTA) results on both datasets. The code and dataset of our work in this research are publicly available in https://github.com/MedlarTea/MPF GRR SLT.\n","date":1651764665,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651764665,"objectID":"59c1968cbd4ff6db1e75c158a6a79e24","permalink":"https://robot-vision-and-navigation-lab.github.io/post/test_news/","publishdate":"2022-05-05T23:31:05+08:00","relpermalink":"/post/test_news/","section":"post","summary":"Following Closely: A Robust Monocular Person Following System for Mobile Robot","tags":[],"title":"A New IROS Article","type":"post"},{"authors":["Ye","Hanjing and Zhao","Jieting and Pan","Yaling and Chen","Weinan and Zhang","Hong"],"categories":null,"content":"","date":1650865709,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650865709,"objectID":"e22af949f8d8bedee9c7ab9ff82031c7","permalink":"https://robot-vision-and-navigation-lab.github.io/publication/following-closely-a-robust-monocular-person-following-system-for-mobile-robot/","publishdate":"2022-04-25T05:48:29.868Z","relpermalink":"/publication/following-closely-a-robust-monocular-person-following-system-for-mobile-robot/","section":"publication","summary":"Monocular person following (MPF) is a capability that supports many useful applications of a mobile robot. However, existing MPF solutions are not completely satisfactory. Firstly, they often fail to track the target at a close distance either because they are based on visual servo or they need the observation of the full body by the robot. Secondly, their target Re-IDentiﬁcation (Re-ID) abilities are weak in cases of target appearance change and highly similar appearance of distracting people. To remove the assumption of full-body observation, we propose a width-based tracking module, which relies on the target width, which can be observed even at a close distance. For handling issues related to appearance variation, we use a global CNN (convolutional neural network) descriptor to represent the target and a ridge regression model to learn a target appearance model online. We adopt a sampling strategy for online classiﬁer learning, in which both long-term and short-term samples are involved. We evaluate our method in two datasets including a public person following dataset and a custom-built with challenging target appearance and target distance. Our method achieves state-of-the-art (SOTA) results on both datasets. The code and dataset of our work in this research are publicly available in https://github.com/MedlarTea/MPF GRR SLT.","tags":null,"title":"Following Closely: A Robust Monocular Person Following System for Mobile Robot","type":"publication"},{"authors":["Ye","Hanjing and Chen","Weinan and Yu","Jingwen and He","Li and Guan","Yisheng and Zhang","Hong"],"categories":null,"content":"","date":1650001649,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650001649,"objectID":"5ba35ac30aec3d688e6dba0e39a016ea","permalink":"https://robot-vision-and-navigation-lab.github.io/publication/condition-invariant-and-compact-visual-place-description-by-convolutional-autoencoder/","publishdate":"2022-04-15T05:47:29.087Z","relpermalink":"/publication/condition-invariant-and-compact-visual-place-description-by-convolutional-autoencoder/","section":"publication","summary":"Visual place recognition (VPR) in condition-varying environments is still an open problem. Popular solutions are CNN-based image descriptors, which have been shown to outperform traditional image descriptors based on hand-crafted visual features. However, there are two drawbacks of current CNN-based descriptors: a) their high dimension and b) lack of generalization, leading to low eﬃciency and poor performance in applications. In this paper, we propose to use a convolutional autoencoder (CAE) to tackle this problem. We employ a high-level layer of a pre-trained CNN to generate features, and train a CAE to map the features to a low-dimensional space to improve the condition invariance property of the descriptor and reduce its dimension at the same time. We verify our method in three challenging datasets involving signiﬁcant illumination changes, and our method is shown to be superior to the state-of-the-art. The code of our work is publicly available in https://github.com/MedlarTea/CAE-VPR.","tags":null,"title":"Condition-Invariant and Compact Visual Place Description by Convolutional Autoencoder","type":"publication"},{"authors":["Chen","Weinan and Zhu","Lei and Lin","Xubin and He","Li and Guan","Yisheng and Zhang","Hong"],"categories":null,"content":"","date":1643694192,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643694192,"objectID":"bb6271cdfc29ba1afa5a460fc903475c","permalink":"https://robot-vision-and-navigation-lab.github.io/publication/dynamic-strategy-of-keyframe-selection-with-pd-controller-for-vslam-systems/","publishdate":"2022-02-01T05:43:12.823Z","relpermalink":"/publication/dynamic-strategy-of-keyframe-selection-with-pd-controller-for-vslam-systems/","section":"publication","summary":"Keyframe (KF) selection in a KF-based visual simultaneous localization and mapping (VSLAM) system is critical. In previous studies, static thresholds have been used for KF selection decision making; however, suboptimal performance can result from the use of such thresholds. To obtain a better KF setting than that obtained with the existing methods, in this article, we introduce a dynamic KF selection strategy. By considering both the view change between camera observation and KFs in the built map and the rate of this change, we propose to dynamically adjust the threshold for KF selection. A proportion and derivative (PD) controller is designed with the feedback of estimated view change, where the PD controller output is used for KF selection. According to the experimental results, compared with the existing studies, our method can improve the precision of visual tracking by 17.5% and 16.7% based on two popular VSLAM systems.","tags":null,"title":"Dynamic Strategy of Keyframe Selection With PD Controller for VSLAM Systems","type":"publication"},{"authors":["Ye","Hanjing and Chen","Guangcheng and Chen","Weinan and He","Li and Guan","Yisheng and Zhang","Hong"],"categories":null,"content":"","date":1638337753,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638337753,"objectID":"519b57fcc449bd84a92ccd9b775fdb06","permalink":"https://robot-vision-and-navigation-lab.github.io/publication/mapping-while-following-2d-lidar-slam-in-indoor-dynamic-environments-with-a-person-tracker/","publishdate":"2021-12-01T05:49:13.498Z","relpermalink":"/publication/mapping-while-following-2d-lidar-slam-in-indoor-dynamic-environments-with-a-person-tracker/","section":"publication","summary":"","tags":null,"title":"Mapping While Following: 2D LiDAR SLAM in Indoor Dynamic Environments with a Person Tracker","type":"publication"},{"authors":["Chen","Weinan and Zhu","Lei and Loo","Shing Yan and Wang","Jiankun and Wang","Chaoqun and Meng","Max Q.-H. and Zhang","Hong"],"categories":null,"content":"","date":1638329974,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638329974,"objectID":"1cb0d6b27efa71de5bc87cf736c4c9e6","permalink":"https://robot-vision-and-navigation-lab.github.io/publication/robustness-improvement-of-using-pre-trained-network-in-visual-odometry-for-on-road-driving/","publishdate":"2021-12-01T03:39:34.551Z","relpermalink":"/publication/robustness-improvement-of-using-pre-trained-network-in-visual-odometry-for-on-road-driving/","section":"publication","summary":"Robustness in on-road driving Visual Odometry (VO) systems is critical, as it determines the reliable performance in various scenarios and environments. Especially with the development of data-driven technology, the combination of data-driven VO and model-based VO has achieved accurate tracking performance. However, the lack of generalization of pre-trained deep neural networks (DNN) limits the robustness of such a combination in unseen environments. In this study, we introduce a novel framework with appropriate usage of DNN prediction and improve the robustness in the self-driving application. Based on the characteristic of on-road self-driving motion and the DNN output, we propose a two-step optimization strategy with a variable degree of freedom (DoF), i.e., the use of two types of DoF representations during pose estimation. Speciﬁcally, our two-step optimization operates according to the residual of the optimization with the motion label classiﬁcation from the pre-trained DNN, as well as our proposed Motion Evaluation by essential matrix construction. Experimental results show that our framework obtains better tracking accuracy than the existing methods.","tags":null,"title":"Robustness Improvement of Using Pre-Trained Network in Visual Odometry for On-Road Driving","type":"publication"},{"authors":["Chen","Weinan and Zhu","Lei and Loo","Shing Yan and Wang","Jiankun and Wang","Chaoqun and Meng","Max Q.-H. and Zhang","Hong"],"categories":null,"content":"","date":1622353461,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622353461,"objectID":"a448c779be61c5e88efeed327bb3b4f9","permalink":"https://robot-vision-and-navigation-lab.github.io/publication/robust-improvement-in-3d-object-landmark-inference-for-semantic-mapping/","publishdate":"2021-05-30T05:44:21.731Z","relpermalink":"/publication/robust-improvement-in-3d-object-landmark-inference-for-semantic-mapping/","section":"publication","summary":"Recent works on semantic Simultaneous Localization and Mapping (SLAM) utilizing object landmarks have shown superiority in terms of robustness and accuracy in tracking and localization. 3D object landmarks represented by a cubic or quadric surface are inferred from 2D object bounding boxes which are typically captured from multiple views by an object detector. Nevertheless, bounding box noises and small camera baseline may lead to an inaccurate 3D object landmark inference. Inspired by the dual quadric enveloping property, in this work, we introduce the horizontal support assumption to constrain rotation w.r.t. roll and pitch for a quadric representation. As the result, we reduce the number of quadric parameters and narrow down the solution space, and ultimately produce a relatively accurate inference. Extensive experimental evaluations under both simulated and real scenarios are conducted in this paper. Quantitative results demonstrate that our approach outperforms the state-of-the-art.","tags":null,"title":"Robust Improvement in 3D Object Landmark Inference for Semantic Mapping","type":"publication"},{"authors":["Chen","Weinan and Zhu","Lei and Wang","Chaoqun and He","Li and Meng","Max Q.-H."],"categories":null,"content":"","date":1621057253,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621057253,"objectID":"0c93708c0db59caa9d2e8941bd654993","permalink":"https://robot-vision-and-navigation-lab.github.io/publication/ceb-map-visual-localization-error-prediction-for-safe-navigation/","publishdate":"2021-05-15T05:40:53.255Z","relpermalink":"/publication/ceb-map-visual-localization-error-prediction-for-safe-navigation/","section":"publication","summary":"For safe visual navigation, areas with high localization errors should be concentrated and could be further reﬁned by additional mapping operations. Given an environment map, we propose to predict the visual localization error and hence to either improve the navigation performance or call an additional mapping to reﬁne the built map. Previous work adopts the uncertainty of landmarks for the error prediction. In our work, we take into account both the spatial distribution of visual landmarks and the uncertainty of landmarks. Our main idea is that standing at one place, a good spatial distribution of landmarks means a sufﬁcient enough visible landmarks from all views at that place, i.e., enough landmarks under arbitrary view-direction. Combining the spatial distribution and the uncertainty of landmarks, we propose a new framework to predict the error of visual localization. Furthermore, we show that additional mapping in the area with high predicted error can signiﬁcantly improve the visual localization precision. Experimental results show that there is a strong relationship between the predicted error and the real error, of which the absolute value of correlation coefﬁcient is between 0.707 to 0.915. We apply our method to conduct an optimal reﬁning policy on the built map and the experimental results show the improved localization precision. Applications on navigation test verify the superiority of our proposed method.","tags":null,"title":"CEB-Map: Visual Localization Error Prediction for Safe Navigation","type":"publication"},{"authors":["Wang","Chaoqun and Ma","Han and Chen","Weinan and Liu","Li and Meng","Max Q.-H."],"categories":null,"content":"","date":1606801574,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606801574,"objectID":"5399b2cb5c45d746a0964d81db9ac50e","permalink":"https://robot-vision-and-navigation-lab.github.io/publication/efficient-autonomous-exploration-with-incrementally-built-topological-map-in-3-d-environments/","publishdate":"2020-12-01T05:46:14.644Z","relpermalink":"/publication/efficient-autonomous-exploration-with-incrementally-built-topological-map-in-3-d-environments/","section":"publication","summary":"Autonomous 3-D exploration with unmanned aerial vehicles (UAVs) is increasingly prevalent for environment monitoring without human intervention. In this article, we present a systematic solution toward efﬁcient UAV exploration in 3-D environments. Innovatively, a road map is incrementally built and maintained along with the exploration process, which explicitly exhibits the topological structure of the 3-D environment. By simplifying the environment, the road map can efﬁciently provide the information gain and the cost-to-go for a candidate region to be explored, which are two quantities for next-best-view (NBV) evaluation, thus prompting the efﬁciency for NBV determination. In addition, with reference to the global plan queried on the road map, we propose a local planner based on the potential ﬁeld method that drives the robot to the information-rich area during the navigation process, which further improves the exploration efﬁciency. The proposed framework and its composed modules are veriﬁed in various 3-D environments, which exhibit their distinctive features in NBV selection and better performance in improving the exploration efﬁciency than other methods.","tags":null,"title":"Efficient Autonomous Exploration With Incrementally Built Topological Map in 3-D Environments","type":"publication"},{"authors":["Dai","Zhuang and Huang","Xinghong and Chen","Weinan and Chen","Chuangbing and He","Li and Wen","Shuhuan and Zhang","Hong"],"categories":null,"content":"","date":1588311910,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588311910,"objectID":"f9f6a67f964ee5cb280a0d2956e6885a","permalink":"https://robot-vision-and-navigation-lab.github.io/publication/keypoint-description-by-descriptor-fusion-using-autoencoders/","publishdate":"2020-05-01T05:45:10.571Z","relpermalink":"/publication/keypoint-description-by-descriptor-fusion-using-autoencoders/","section":"publication","summary":"Keypoint matching is an important operation in computer vision and its applications such as visual simultaneous localization and mapping (SLAM) in robotics. This matching operation heavily depends on the descriptors of the keypoints, and it must be performed reliably when images undergo conditional changes such as those in illumination and viewpoint. In this paper, a descriptor fusion model (DFM) is proposed to create a robust keypoint descriptor by fusing CNN-based descriptors using autoencoders. Our DFM architecture can be adapted to either trained or pre-trained CNN models. Based on the performance of existing CNN descriptors, we choose HardNet and DenseNet169 as representatives of trained and pre-trained descriptors. Our proposed DFM is evaluated on the latest benchmark datasets in computer vision with challenging conditional changes. The experimental results show that DFM is able to achieve state-of-the-art performance, with the mean mAP that is 6.45% and 6.53% higher than HardNet and DenseNet169, respectively.","tags":null,"title":"Keypoint Description by Descriptor Fusion Using Autoencoders","type":"publication"},{"authors":["Zhu","Lei and Chen","Weinan and Lin","Xubin and He","Li and Guan","Yisheng and Zhang","Hong"],"categories":null,"content":"","date":1575179409,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575179409,"objectID":"2ded2389ab7ff9edc866c8341679cff9","permalink":"https://robot-vision-and-navigation-lab.github.io/publication/random-walk-network-for-3d-point-cloud-classification-and-segmentation/","publishdate":"2019-12-01T05:50:09.719Z","relpermalink":"/publication/random-walk-network-for-3d-point-cloud-classification-and-segmentation/","section":"publication","summary":"Object classiﬁcation and segmentation via point cloud are essential for mobile robot navigation and operation. A lot of researches ranging from 3D voxels, mesh gird and multi-view were proposed based on point cloud. However, an accurate point cloud classiﬁcation is still a challenging problem. Inspired by multi-label classiﬁcation in images and convolutional neural networks (CNN), in this paper we present a novel network, named Random Walk Network (RWNet), which directly processes raw 3D point cloud data to classify points and, as a result, segment one 3D scene. State-of-theart methods mainly focus on the features of one point while spatial relationships are also essential in point classiﬁcation. To deal with this issue, we combine both appearance features and spatial information of feature points to restrain the point cloud processing. We employ PointNet ﬁrst to generate initial point labels and adopt point labels with high conﬁdence as seeds. We then construct the similarity matrix between seeds and low-conﬁdence-labeled points according to their structural and spatial similarity and use Random Walk to obtain the ﬁnal classiﬁcation. We demonstrate our method in 3D classiﬁcation task in various scenes and compare with some benchmark methods. Experimental results show that RWNet has a better performance than others.","tags":null,"title":"Random Walk Network for 3D Point Cloud Classification and Segmentation","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://robot-vision-and-navigation-lab.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://robot-vision-and-navigation-lab.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f1d044c0738ab9f19347f15c290a71a1","permalink":"https://robot-vision-and-navigation-lab.github.io/research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/","section":"","summary":"","tags":null,"title":"Research Highlights","type":"widget_page"}]